The dataset for this project is the NELA-GT-2018 dataset from the Harvard Dataverse. The features for the data include the publication date, news source, article headline, as well as article content. It was created by scraping the articles from an RSS feed or web page. Included in the database are news articles from 194 unique sources, totaling 713,534 articles. Examples of these news sources are CNN, Fox News, and MSNBC. As the data involves an extensive amount of text data it will provide us the opportunity to perform text preprocessing and manipulation, explore various natural language processing algorithms to evaluate sentiments and topic models, draw insights through clustering, and sentence/text generation.

This dataset will require standard text preprocessing. The first step will be to remove punctuation, stop words, excess white space, numbers. The words will all be made lowercase, and lemmatization will be used to reduce all words to their respective roots. The text will be transformed into a TF-IDF matrix. All of the analysis we perform will use this TF-IDF matrix. 
There are several questions we would like to answer using this dataset. Firstly, are news sources more prone to a positive or negative sentiment? For this we will perform sentiment analysis to determine which news sources are positive or negative. This is an interesting application of text analysis because people tend to get a lot of their news from a single or small group of sources. It will be interesting to see the overall common sentiment for each news source, especially surrounding specific events. 
Secondly, are there any common topics that are discussed by the most prominent news sources? Can news sources be clustered together based on similarities or differences in their articles or headlines? For example, what are the topics most commonly discussed by Fox News, and are these topics different from those commonly discussed by CNN? We would like to determine if there are common topics discussed by all news sources over specified time periods, such as the presidential election. Topic modeling with Latent Dirichlet Allocation will be used for our analyses as well as singular value decomposition when necessary. This analysis should illustrate which topics different news sources may deem important and worthy of coverage, as well as potential differing sentiments toward similar topics. 
This would be useful in determining if a lot of news sources are redundant, or if different styles of articles or different information can be learned. To determine this, clustering will be used on the dataset TF-IDF. T-SNE plots will provide visualizations of clusters, while we can utilize different algorithms including hierarchical clustering, GMM, and k-means. The analysis will include comparisons of the most relevant topics split by news source. This would allow us to determine which algorithm may best cluster the text of this dataset. After performing clustering, it will be interesting to see if the sources clustered together by their articles are those we expect from real world experience, or if our expectations are subverted. 
A stretch goal of ours will be to create a text/sentence generation model based on the news headlines and article content from the data. For this goal we will utilize different neural network libraries and TensorFlow to create an application which will allow us to provide text input to the model. This model will subsequently use probabilities based on prior articles to construct headlines using the given text. To do this, a Long short-term memory (LSTM) neural network will be trained using the text dataset. This model will be used to predict a set of words that will hopefully generate a headline or sentence. This application is useful because we want to see if it is possible to train a model that can create headlines or sentences that are indistinguishable from human written text. 

